{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 2 — Model Development & Hyperparameter Tuning\n",
        "\n",
        "**Student:** Aditya Goyal | **ID:** iitp_aiml_2506120\n",
        "\n",
        "This notebook covers:\n",
        "- **Task 2.1** — Build 4 Classification Models (Logistic Regression, Decision Tree, Random Forest, XGBoost)\n",
        "- **Task 2.2** — Hyperparameter Tuning with GridSearchCV (5-fold CV) on at least 2 models\n",
        "- **Task 2.3** — Experiment Tracking with MLflow (separate run per model, metrics + confusion matrix artifact)\n",
        "- **Task 2.4** — Save the best model and scaler to Google Drive for use in Notebook 3"
      ],
      "metadata": {
        "id": "I24xlyIrh3ob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTme6NxmkWDH",
        "outputId": "02c6e868-56d4-4196-eb2e-b2ea162ff5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set Project Path (Must match Notebook 1)\n",
        "project_path = '/content/drive/MyDrive/T2_Project_Aditya_Goyal'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the datasets\n",
        "train_df = pd.read_csv(f'{project_path}/Data/X_train.csv')\n",
        "test_df = pd.read_csv(f'{project_path}/Data/X_test.csv')\n",
        "\n",
        "# 2. FOR XGBOOST: Convert 'good'/'bad' to 0/1\n",
        "target_mapping = {'good': 0, 'bad': 1}\n",
        "train_df['Risk'] = train_df['Risk'].map(target_mapping)\n",
        "test_df['Risk'] = test_df['Risk'].map(target_mapping)\n",
        "\n",
        "# 3. Define X and y for training and testing\n",
        "X_train = train_df.drop('Risk', axis=1)\n",
        "y_train = train_df['Risk']\n",
        "X_test = test_df.drop('Risk', axis=1)\n",
        "y_test = test_df['Risk']\n",
        "\n",
        "print(f\"Data Loaded and Target Encoded. X_train shape: {X_train.shape}\")\n"
      ],
      "metadata": {
        "id": "wZlxl-0Qk1CL",
        "outputId": "0c495d24-5768-4ce6-8958-ab1207561dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded and Target Encoded. X_train shape: (800, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.1 Build 4 Classification Models\n",
        "\n",
        "Train four classification models on the preprocessed training data:\n",
        "\n",
        "| Model | Type | Notes |\n",
        "|---|---|---|\n",
        "| Logistic Regression | Linear (Baseline) | With `StandardScaler` pipeline |\n",
        "| Decision Tree | Non-linear | Default depth for baseline |\n",
        "| Random Forest | Ensemble (Bagging) | 100 estimators |\n",
        "| XGBoost | Ensemble (Boosting) | `eval_metric='logloss'` |"
      ],
      "metadata": {
        "id": "g1XLLS8viELV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Logistic Regression (Baseline)\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Baseline models trained successfully.\")"
      ],
      "metadata": {
        "id": "HBMwnxTcnBf7",
        "outputId": "a91830cc-455a-431a-d086-23885cc227d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline models trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# 3. Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. XGBoost\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Advanced models (Random Forest & XGBoost) trained successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCRUr720ouw",
        "outputId": "61b18350-b8d2-4cdb-a8ed-85a4c3d78252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced models (Random Forest & XGBoost) trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.2 Hyperparameter Tuning — At Least 2 Models\n",
        "\n",
        "Use `GridSearchCV` with **5-fold cross-validation** to tune hyperparameters. Scoring metric: **F1** (better than accuracy for imbalanced credit-risk data).\n",
        "\n",
        "**Models tuned:**\n",
        "- **Random Forest** — `n_estimators`, `max_depth`, `min_samples_split`, `criterion`\n",
        "- **XGBoost** — `n_estimators`, `learning_rate`\n",
        "\n",
        "Document the best parameters found and compare pre- vs post-tuning F1 performance below."
      ],
      "metadata": {
        "id": "OBjR8xJGiOsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Define the parameter grid to search\n",
        "# We will test different numbers of trees and depths\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# 2. Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='f1', # F1 is better for credit risk than accuracy\n",
        "                           n_jobs=-1)\n",
        "\n",
        "# 3. Fit the search to your training data\n",
        "print(\"Starting Grid Search... this may take a minute.\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 4. Results\n",
        "print(f\"Best Parameters Found: {grid_search.best_params_}\")\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "# 5. Test the tuned model\n",
        "tuned_preds = best_rf_model.predict(X_test)\n",
        "print(f\"Tuned Random Forest F1 Score: {f1_score(y_test, tuned_preds):.4f}\") # Changed to f1_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAP6b59Q2a3u",
        "outputId": "b9f19d37-a96b-4475-84b9-4f564470d1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Grid Search... this may take a minute.\n",
            "Best Parameters Found: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
            "Tuned Random Forest F1 Score: 0.7805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Tuning Results — Pre vs Post Comparison\n",
        "\n",
        "**Best Parameters (Random Forest):**\n",
        "- `criterion`: entropy\n",
        "- `max_depth`: 10\n",
        "- `min_samples_split`: 5\n",
        "- `n_estimators`: 200\n",
        "\n",
        "| Model | Pre-Tuning F1 | Post-Tuning F1 | Improvement |\n",
        "|---|---|---|---|\n",
        "| Random Forest | ~0.74 (default) | **0.7805** | +0.04 |\n",
        "| XGBoost | ~0.72 (default) | Logged via MLflow | See Section 2.3 |\n",
        "\n",
        "**Observation:** Tuning `max_depth=10` prevents overfitting while `entropy` criterion captures better information gain for this credit-risk dataset. The 5-fold CV ensures the improvement generalises beyond the training set."
      ],
      "metadata": {
        "id": "KaHZeN3Ciaw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.3 Experiment Tracking with MLflow\n",
        "\n",
        "Set up **MLflow** to track every model run with:\n",
        "- A **separate run** for each model variant (descriptive `run_name`)\n",
        "- All **hyperparameters** logged via `mlflow.log_param` / `mlflow.log_params`\n",
        "- All **metrics** logged: `accuracy`, `f1_score`, `precision`, `recall`\n",
        "- **Confusion matrix** saved and logged as an artifact via `mlflow.log_artifact`\n",
        "- **Model artifact** logged via `mlflow.sklearn.log_model`\n",
        "\n",
        "Experiment name: `credit_risk_classification`\n",
        "Runs: `random_forest_tuned`, `xgboost_tuned`"
      ],
      "metadata": {
        "id": "KI2NM09ViiNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Setup MLflow Experiment\n",
        "mlflow.set_experiment(\"credit_risk_classification\")\n",
        "\n",
        "def log_model_run(model, run_name, model_type, params, X_test, y_test):\n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # Log Hyperparameters\n",
        "        mlflow.log_param(\"model_type\", model_type)\n",
        "        mlflow.log_params(params)\n",
        "\n",
        "        # Calculate and Log Metrics\n",
        "        y_pred = model.predict(X_test)\n",
        "        mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
        "        mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
        "        mlflow.log_metric(\"precision\", precision_score(y_test, y_pred, zero_division=0))\n",
        "        mlflow.log_metric(\"recall\",    recall_score(y_test, y_pred, zero_division=0))\n",
        "\n",
        "        # Create and Log Confusion Matrix as an Artifact\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix - {run_name}')\n",
        "        plt.savefig(\"confusion_matrix.png\")\n",
        "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Log the Model\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "        print(f\"Logged {run_name} to MLflow.\")\n",
        "\n",
        "# Log Tuned Random Forest\n",
        "log_model_run(best_rf_model, \"random_forest_tuned\", \"Random Forest\", grid_search.best_params_, X_test, y_test)\n",
        "\n",
        "# Tune and Log XGBoost (Task 2.2 - Second Model)\n",
        "xgb_grid = GridSearchCV(estimator=xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "                        param_grid={'n_estimators': [100, 200], 'learning_rate': [0.1, 0.2]},\n",
        "                        cv=5, scoring='f1', n_jobs=-1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "log_model_run(xgb_grid.best_estimator_, \"xgboost_tuned\", \"XGBoost\", xgb_grid.best_params_, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO5QAda_Riqw",
        "outputId": "35c08d25-9628-46ca-f62c-7f140ce89afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/19 07:37:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged random_forest_tuned to MLflow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/19 07:38:04 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged xgboost_tuned to MLflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vMW0XDNvP3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2.4 Save Best Model & Scaler (for Notebook 3)\n",
        "\n",
        "Serialise the best-tuned Random Forest and the fitted `StandardScaler` to Google Drive using `joblib`. These artefacts will be loaded in **Notebook 3** for final evaluation.\n",
        "\n",
        "- **Model path:** `Models/best_random_forest_model.pkl`\n",
        "- **Scaler path:** `Models/scaler.pkl`\n",
        "\n",
        "> The scaler is re-fit on the **numeric columns only** of `X_train` to ensure consistent preprocessing when scoring new data in Notebook 3."
      ],
      "metadata": {
        "id": "rbs7YxMUivA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Re-defining num_cols\n",
        "num_cols = ['Age', 'Credit amount', 'Duration', 'Repayment_Strain', 'Liquidity_Score', 'Installment rate']\n",
        "\n",
        "# Re-initialize and fit the scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[num_cols])\n",
        "\n",
        "print(\"Scaler defined and fitted successfully.\")"
      ],
      "metadata": {
        "id": "csP0l1PS4bNo",
        "outputId": "408260f4-c07e-4848-c118-a6d29cdfe2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler defined and fitted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create the Models directory if it doesn't exist\n",
        "models_dir = f'{project_path}/Models'\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "    print(f\"Created directory: {models_dir}\")\n",
        "\n",
        "# Saving the Best Tuned Random Forest Model\n",
        "model_filename = f'{project_path}/Models/best_random_forest_model.pkl'\n",
        "joblib.dump(best_rf_model, model_filename)\n",
        "\n",
        "# Saving the Scaler (Required for Notebook 3 to process new data)\n",
        "joblib.dump(scaler, f'{project_path}/Models/scaler.pkl')\n",
        "\n",
        "print(f\"Model and Scaler saved successfully to: {project_path}/Models/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFAbDyS_3qxm",
        "outputId": "165eee94-dc86-4966-e13a-c5211a24e34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and Scaler saved successfully to: /content/drive/MyDrive/T2_Project_Aditya_Goyal/Models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model,          Best Hyperparameters                                                                      Found,F1-Score\n",
        "#Random Forest,  \"criterion: entropy, max_depth: 10, min_samples_split: 5, n_estimators: 200\",            0.7805\n",
        "#XGBoost,         \"learning_rate: 0.1, n_estimators: 100\",                                                 0.7619\n",
        "#Conclusion: Hyperparameter tuning using 5-fold cross-validation successfully optimized the Random Forest model to its best performance of 0.7805 F1-score."
      ],
      "metadata": {
        "id": "LsJiPjWLTfR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}